# GPT2 CUDA
This project is an implementation of the GPT2 model using C/CUDA. The main focus is to be able to implement this model exploring and abusing the computational power of GPUs and apply CUDA concepts/techniques making the most efficient as possible. Also, one future feature is to perform a quantized traning, post-training quantization and eventually a multi-GPU training.  

This project is heavily inspired by llm.c by karpathy (https://github.com/karpathy/llm.c). The main structure of the pure C implementation was based on llm.c, as a resource to follow along.